{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f74a8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading base model with 4-bit quantization...\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,162,688 || all params: 496,195,456 || trainable%: 0.4359\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15052f632cbf44fb9f26afe88a024625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/34772 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunuru/.local/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "INFO:__main__:ðŸš€ Starting fine-tuning...\n",
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3258' max='3258' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3258/3258 12:53:49, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.998200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.894800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.844600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.818600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.788200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.764500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.765800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.751400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.744400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.719800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.702900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.702400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.704400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.676600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.694600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.696900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.664600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.691000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.668300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.665500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.669200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/kunuru/.local/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "INFO:__main__:âœ… Fine-tuning completed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Fine-tuned Qwen2.5 mental health chatbot is ready!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoConfig,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "MODEL_SAVE_PATH = \"./qwen2.5-0.5B_finetuned_mentalhealth\"\n",
    "DATA_FILE = \"cleaned_data27.csv\"\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "MAX_LENGTH = 384  \n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "\n",
    "# LoRA config\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load model (base or already fine-tuned)\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    logger.info(\"Loading fine-tuned model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_SAVE_PATH,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "else:\n",
    "    logger.info(\"Loading base model with 4-bit quantization...\")\n",
    "    config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    if hasattr(config, \"quantization_config\"):\n",
    "        del config.quantization_config\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        config=config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Optional: Try flash attention if supported (uncomment to test)\n",
    "    # model.config.attn_implementation = \"flash_attention_2\"\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # Inject LoRA adapters\n",
    "    peft_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "# Format dataset\n",
    "def format_conversation(row):\n",
    "    return f\"User: {row['Context']}\\nTherapist: {row['Response']}{tokenizer.eos_token}\"\n",
    "\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "df['text'] = df.apply(format_conversation, axis=1)\n",
    "# Optional: Use subset for faster testing\n",
    "# df = df.sample(n=1000)\n",
    "\n",
    "dataset = Dataset.from_pandas(df[['text']])\n",
    "\n",
    "# Tokenization (don't return tensors)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['text'])\n",
    "\n",
    "# Data collator (handles dynamic padding + labels)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_SAVE_PATH,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    bf16=True,  # Use bf16 if supported, otherwise switch to fp16=True\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"no\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train\n",
    "logger.info(\"ðŸš€ Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "logger.info(\"âœ… Fine-tuning completed!\")\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "print(\"\\nâœ… Fine-tuned Qwen2.5 mental health chatbot is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d1ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hi\n",
      "Bot: Hello! I'm here to listen. How are you feeling today?\n",
      "You: i am going through a rough time . i am not able to concentrate on my work\n",
      "Bot: it sounds like you might be experiencing some challenges with your focus and productivity at home. can we try talking about any recent changes or developments in your life that could be contributing to these difficulties? also do you have any specific concerns about your work environment or responsibilities that need more attention regarding concentration and efficiency. if so please let me know. as a mental health professional, I would be happy to help analyze the situation further and provide suggestions for improving your productivity and focus at home while also considering possible external factors such as stress levels or other personal issues that may be impacting your ability to concentrate. remember though it is important to approach this conversation with empathy and understanding as well as an open mind to exploring potential solutions together. take care.\n",
      "You: i wanted to die today\n",
      "Bot: please do not hesitate to reach out for help if you need it. remember that there is always hope and support available. your feelings and thoughts deserve attention even though they may be difficult to express. take care of yourself as much as possible during this time. how can i assist you further?\n",
      "You: i am going to die today\n",
      "Bot: i understand that you might be feeling very anxious and uncertain right now. it is important to remember that every person has the power to control their own destiny. if you have any questions or concerns about your treatment plan please let me know. do not hesitate to reach out for support when you feel overwhelmed by these feelings. your mental health matters greatly and i will always be here to assist you in whatever way possible. how can i help with this?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model_name = \"qwen2.5-0.5B_finetuned_mentalhealth\"  # Update this with your actual model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = model.device\n",
    "\n",
    "# Generation parameters\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "TOP_K = 50\n",
    "MAX_NEW_TOKENS = 150\n",
    "\n",
    "# Base system prompt\n",
    "SYSTEM_PROMPT = \"You are a compassionate mental health support chatbot. Respond empathetically to user messages.\"\n",
    "\n",
    "# Optional greeting responses\n",
    "GREETING_RESPONSES = [\n",
    "    \"Hello! I'm here to listen. How are you feeling today?\",\n",
    "    \"Hi there! I'm here if you need someone to talk to.\",\n",
    "    \"Hey! I'm glad you're here. How are things going for you?\"\n",
    "]\n",
    "\n",
    "# Chat loop\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
    "            print(\"Bot: Goodbye! Take care.\")\n",
    "            break\n",
    "\n",
    "        # Handle greetings manually\n",
    "        if user_input.lower() in [\"hi\", \"hello\", \"hey\"]:\n",
    "            print(f\"Bot: {GREETING_RESPONSES[0]}\")\n",
    "            continue\n",
    "\n",
    "        # Build full input prompt\n",
    "        prompt = f\"{SYSTEM_PROMPT}\\nUser: {user_input}\\nBot:\"\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=True,\n",
    "                temperature=TEMPERATURE,\n",
    "                top_p=TOP_P,\n",
    "                top_k=TOP_K,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "\n",
    "        # Decode and postprocess\n",
    "        decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract response part only\n",
    "        if \"Bot:\" in decoded_output:\n",
    "            response = decoded_output.split(\"Bot:\")[-1].strip()\n",
    "        else:\n",
    "            response = decoded_output.strip()\n",
    "\n",
    "        print(f\"Bot: {response}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nBot: Session ended. Wishing you well.\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Bot: I encountered an error. Let's try again. ({str(e)})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3540df",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41fdbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Generating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:41<00:00,  4.15s/it]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… ROUGE Scores:\n",
      "  rouge1: 0.3355\n",
      "  rouge2: 0.0818\n",
      "  rougeL: 0.1565\n",
      "  rougeLsum: 0.1962\n",
      "\n",
      "âœ… BERTScore:\n",
      "  Precision: 0.8532\n",
      "  Recall:    0.8589\n",
      "  F1:        0.8558\n"
     ]
    }
   ],
   "source": [
    "!pip install -q evaluate bert_score transformers\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_path = \"./qwen2.5-0.5B_finetuned_mentalhealth\"  # or \"./falcon3b-lora-checkpoint\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load test dataset\n",
    "df = pd.read_csv(\"cleaned_data27.csv\").dropna().sample(n=10, random_state=4)\n",
    "contexts = df[\"Context\"].tolist()\n",
    "references = df[\"Response\"].tolist()\n",
    "\n",
    "generated_responses = []\n",
    "for context in tqdm(contexts, desc=\"Generating\"):\n",
    "    input_text = f\"User: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated_responses.append(decoded.split(\"Bot:\")[-1].strip() if \"Bot:\" in decoded else decoded)\n",
    "\n",
    "# Evaluate\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "rouge_results = rouge.compute(predictions=generated_responses, references=references)\n",
    "bert_results = bertscore.compute(predictions=generated_responses, references=references, lang=\"en\")\n",
    "\n",
    "print(f\"\\nâœ… ROUGE Scores:\")\n",
    "for k, v in rouge_results.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "print(f\"\\nâœ… BERTScore:\")\n",
    "print(f\"  Precision: {sum(bert_results['precision']) / len(bert_results['precision']):.4f}\")\n",
    "print(f\"  Recall:    {sum(bert_results['recall']) / len(bert_results['recall']):.4f}\")\n",
    "print(f\"  F1:        {sum(bert_results['f1']) / len(bert_results['f1']):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86989a08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
